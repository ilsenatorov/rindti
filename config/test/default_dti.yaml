datamodule:
  batch_size: 8
  num_workers: 0
model:
  monitor: train_loss
  feat_method: [element_l1, concat]
  drug:
    method: graph
    hidden_dim: 16
    node:
      module: [ginconv, gatconv]
      hidden_dim: 16
    pool:
      module: [diffpool, gmt]
      hidden_dim: 16
      num_heads: 1

  prot:
    method: graph
    hidden_dim: 16
    node:
      module: ginconv
      hidden_dim: 16
    pool:
      module: gmt
      hidden_dim: 16
  mlp:
    hidden_dim: 16
    num_layers: 4
  optimizer:
    module: adam
    lr: 0.0001
    prot_lr: 0.0001
    drug_lr: 0.0001
    weight_decay: 0.01
    momentum: 0.01
    reduce_lr:
      factor: 0.1
      patience: 20
      monitor: train_loss
trainer:
  max_epochs: 10

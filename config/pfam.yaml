data: test.pkl
# Hardware
batch_size: 64
gpus: 1
max_epochs: 1000
num_workers: 16
profiler: null
seed: 47
# Optimisation + learning
early_stop_patience: 200
gradient_clip_val: 100
lr: 0.001
monitor: train_loss
optimiser: adamw
reduce_lr_factor: 0.1
reduce_lr_patience: 50
# Model
dropout: 0.1
hidden_dim: 64
model: pfam
batch_per_epoch: 1000
prot_per_fam: 16
margin: 1
temp: 0.5
optim_temp: True
loss: snnl # lifted or snnl
node_pred: True
frac: 0.2
alpha: 0.1
# Encoder
node_embed: transformer
num_heads: 4
num_layers: 3
pool: gmt
ratio: 0.25
